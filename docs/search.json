[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is to document journey in Generative Artificial Intelligence technology."
  },
  {
    "objectID": "posts/llm_inference_using_rtx_2080_ti-part_2/index.html",
    "href": "posts/llm_inference_using_rtx_2080_ti-part_2/index.html",
    "title": "LLM Inference using RTX 2080 Ti (Part 2)",
    "section": "",
    "text": "Include LangChain framework so that the LLM application is context aware and perform reasoning based on language model selected.\n\n\n\n\nContinue using Python Virtual Environment Meta - Llama2 model from Part 1.\nInstall langchain python package\npip install langchain\nUse the code from Part 1 and add langchain packages:\n\nImport additional component - “pipeline” in transformers\nReference langchain package\nConvert to “prompt”\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\nfrom langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\nfrom langchain.prompts import PromptTemplate\n\n# load model in 4-bit\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n# pre-downloaded model\nmodel_name = \"./Llama-2-7b-chat-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             quantization_config=quantization_config)\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10)\n\nhf = HuggingFacePipeline(pipeline=pipe)\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\nprompt = PromptTemplate.from_template(template)\n\nchain = prompt | hf\n\nquestion = \"What is electroencephalography?\"\nprint(chain.invoke({\"question\": question}))\nOutput\n\nPlayaround “max_new_tokens” value in pipeline to see the changes of output. eg. max_new_tokens=2000\n\n\n\n\n\n\nhttps://python.langchain.com/docs/integrations/llms/huggingface_pipelines"
  },
  {
    "objectID": "posts/llm_inference_using_rtx_2080_ti-part_2/index.html#overview",
    "href": "posts/llm_inference_using_rtx_2080_ti-part_2/index.html#overview",
    "title": "LLM Inference using RTX 2080 Ti (Part 2)",
    "section": "",
    "text": "Include LangChain framework so that the LLM application is context aware and perform reasoning based on language model selected."
  },
  {
    "objectID": "posts/llm_inference_using_rtx_2080_ti-part_2/index.html#steps",
    "href": "posts/llm_inference_using_rtx_2080_ti-part_2/index.html#steps",
    "title": "LLM Inference using RTX 2080 Ti (Part 2)",
    "section": "",
    "text": "Continue using Python Virtual Environment Meta - Llama2 model from Part 1.\nInstall langchain python package\npip install langchain\nUse the code from Part 1 and add langchain packages:\n\nImport additional component - “pipeline” in transformers\nReference langchain package\nConvert to “prompt”\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\nfrom langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\nfrom langchain.prompts import PromptTemplate\n\n# load model in 4-bit\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n# pre-downloaded model\nmodel_name = \"./Llama-2-7b-chat-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             quantization_config=quantization_config)\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10)\n\nhf = HuggingFacePipeline(pipeline=pipe)\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\nprompt = PromptTemplate.from_template(template)\n\nchain = prompt | hf\n\nquestion = \"What is electroencephalography?\"\nprint(chain.invoke({\"question\": question}))\nOutput\n\nPlayaround “max_new_tokens” value in pipeline to see the changes of output. eg. max_new_tokens=2000"
  },
  {
    "objectID": "posts/llm_inference_using_rtx_2080_ti-part_2/index.html#references",
    "href": "posts/llm_inference_using_rtx_2080_ti-part_2/index.html#references",
    "title": "LLM Inference using RTX 2080 Ti (Part 2)",
    "section": "",
    "text": "https://python.langchain.com/docs/integrations/llms/huggingface_pipelines"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a blog. Welcome!"
  },
  {
    "objectID": "posts/llm_inference_using_rtx_2080_ti-part_3/index.html",
    "href": "posts/llm_inference_using_rtx_2080_ti-part_3/index.html",
    "title": "LLM Inference using RTX 2080 Ti (Part 3)",
    "section": "",
    "text": "Build a simple question-answering application using LangChain. This time, a vector-store component is needed for computation and FAISS developed by Meta is picked to try out. As FAISS only support up to python version 3.10 and official package only available from conda, hence switching development environment is performed. Intel python bundle is selected since the current device is intel based and it have accelerated python packages dedicated for Intel cpu and have conda package management needed to install FAISS.\n\n\n\n\n\n\nCategory\nModel\n\n\n\n\nLanguage\nPython 3.10.13\n\n\n\n\n\n\n\nSetup Intel Python environment. Download from Intel website and install it from bash. Follow the interactive question to setup the initial environment. The package management will be “conda” instead “pip”.\n# https://www.intel.com/content/www/us/en/developer/articles/technical/get-started-with-intel-distribution-for-python.html\nbash ./intelpython3-2024.0.0_251-Linux-x86_64.sh\nAdditional Conda Setup\n\nDeactivate auto activate as “base” when open terminal\nUse mamba library as resolver to speed-up dependency resolver.\n\n# no auto activate \"base\" conda\nconda config --set auto_activate_base false\n\n# Use new solver\nconda update -n base conda\nconda install -n base conda-libmamba-solver\nconda config --set solver libmamba    \nSetup new environment with python 3.10 as default python version come with is 3.9, activate the environment and re-install python packages used in previous lessons.\n# create new environment named \"hf\"\nconda create -n hf python=3.10\n\nconda activate hf\n\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n\nconda install bitsandbytes transformers accelerate optimum\nconda install langchain\nInstall new python packages need for data loading and processing:\n\nFAISS vector stores for vector processing,\nsentence-transformers used in HuggingFace embedding for computing, and\nbeautifulsoup4 for webpage processing python packages.\n\nconda install faiss-gpu\nconda install sentence-transformers beautifulsoup4\nBased on sample code from Langchain - Use Cases - Q&A with RAG - Quickstart:\n\nUse HuggingFace model instead of OpenAI\n# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\nllm = HuggingFacePipeline(pipeline=pipe)\nChange usage of Chroma to FAISS, and OpenAiEmbeddings to HuggingFaceEmbeddings\n# vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\nvectorstore = FAISS.from_documents(documents=splits, embedding=HuggingFaceEmbeddings())\nConvert template in Hub to “ChatPromptTemplate” and “HumanMessagePromptTemplate” equivalent.\n# prompt = hub.pull(\"rlm/rag-prompt\")\nprompt = ChatPromptTemplate.from_messages(\n    [\n        HumanMessagePromptTemplate.from_template(\n\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:\"\"\"\n        ),\n    ]\n)\nComment out vectorstore.delete_collection() as FAISS package do not have delete collection function.\n# vectorstore.delete_collection()\n\nOutput\n\n\n\n\n\n\nhttps://www.intel.com/content/www/us/en/developer/articles/tool/oneapi-standalone-components.html#python\nhttps://github.com/facebookresearch/faiss\nhttps://python.langchain.com/docs/use_cases/question_answering/quickstart"
  },
  {
    "objectID": "posts/llm_inference_using_rtx_2080_ti-part_3/index.html#overview",
    "href": "posts/llm_inference_using_rtx_2080_ti-part_3/index.html#overview",
    "title": "LLM Inference using RTX 2080 Ti (Part 3)",
    "section": "",
    "text": "Build a simple question-answering application using LangChain. This time, a vector-store component is needed for computation and FAISS developed by Meta is picked to try out. As FAISS only support up to python version 3.10 and official package only available from conda, hence switching development environment is performed. Intel python bundle is selected since the current device is intel based and it have accelerated python packages dedicated for Intel cpu and have conda package management needed to install FAISS."
  },
  {
    "objectID": "posts/llm_inference_using_rtx_2080_ti-part_3/index.html#environment",
    "href": "posts/llm_inference_using_rtx_2080_ti-part_3/index.html#environment",
    "title": "LLM Inference using RTX 2080 Ti (Part 3)",
    "section": "",
    "text": "Category\nModel\n\n\n\n\nLanguage\nPython 3.10.13"
  },
  {
    "objectID": "posts/llm_inference_using_rtx_2080_ti-part_3/index.html#steps",
    "href": "posts/llm_inference_using_rtx_2080_ti-part_3/index.html#steps",
    "title": "LLM Inference using RTX 2080 Ti (Part 3)",
    "section": "",
    "text": "Setup Intel Python environment. Download from Intel website and install it from bash. Follow the interactive question to setup the initial environment. The package management will be “conda” instead “pip”.\n# https://www.intel.com/content/www/us/en/developer/articles/technical/get-started-with-intel-distribution-for-python.html\nbash ./intelpython3-2024.0.0_251-Linux-x86_64.sh\nAdditional Conda Setup\n\nDeactivate auto activate as “base” when open terminal\nUse mamba library as resolver to speed-up dependency resolver.\n\n# no auto activate \"base\" conda\nconda config --set auto_activate_base false\n\n# Use new solver\nconda update -n base conda\nconda install -n base conda-libmamba-solver\nconda config --set solver libmamba    \nSetup new environment with python 3.10 as default python version come with is 3.9, activate the environment and re-install python packages used in previous lessons.\n# create new environment named \"hf\"\nconda create -n hf python=3.10\n\nconda activate hf\n\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n\nconda install bitsandbytes transformers accelerate optimum\nconda install langchain\nInstall new python packages need for data loading and processing:\n\nFAISS vector stores for vector processing,\nsentence-transformers used in HuggingFace embedding for computing, and\nbeautifulsoup4 for webpage processing python packages.\n\nconda install faiss-gpu\nconda install sentence-transformers beautifulsoup4\nBased on sample code from Langchain - Use Cases - Q&A with RAG - Quickstart:\n\nUse HuggingFace model instead of OpenAI\n# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\nllm = HuggingFacePipeline(pipeline=pipe)\nChange usage of Chroma to FAISS, and OpenAiEmbeddings to HuggingFaceEmbeddings\n# vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\nvectorstore = FAISS.from_documents(documents=splits, embedding=HuggingFaceEmbeddings())\nConvert template in Hub to “ChatPromptTemplate” and “HumanMessagePromptTemplate” equivalent.\n# prompt = hub.pull(\"rlm/rag-prompt\")\nprompt = ChatPromptTemplate.from_messages(\n    [\n        HumanMessagePromptTemplate.from_template(\n\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:\"\"\"\n        ),\n    ]\n)\nComment out vectorstore.delete_collection() as FAISS package do not have delete collection function.\n# vectorstore.delete_collection()\n\nOutput"
  },
  {
    "objectID": "posts/llm_inference_using_rtx_2080_ti-part_3/index.html#references",
    "href": "posts/llm_inference_using_rtx_2080_ti-part_3/index.html#references",
    "title": "LLM Inference using RTX 2080 Ti (Part 3)",
    "section": "",
    "text": "https://www.intel.com/content/www/us/en/developer/articles/tool/oneapi-standalone-components.html#python\nhttps://github.com/facebookresearch/faiss\nhttps://python.langchain.com/docs/use_cases/question_answering/quickstart"
  },
  {
    "objectID": "posts/llm_inference_using_rtx_2080_ti-part_1/index.html",
    "href": "posts/llm_inference_using_rtx_2080_ti-part_1/index.html",
    "title": "LLM Inference using RTX 2080 Ti (Part 1)",
    "section": "",
    "text": "First try on Large Language Model (LLM) inference coding. Pick Llama2 7b Chat HF model which can downloaded from HuggingFace after request from Meta. Inference hardware is using Nvidia RTX 2080 Ti and code in python language. Primary Python packages are from HuggingFace packages which are transformer, accelerate, optimum and bitsandbytes. Due to limitation of RTX 2080 Ti Turing architecture and available of GPU memory size, the model need to quantize to lower bit so that it can fit into 2080 Ti memory size and exclude of usage FlashAttention2 packages since it only support Ampere and newer.\n\n\n\n\n\n\nCategory\nModel\n\n\n\n\nCPU\nIntel i7-8700K\n\n\nGPU\nNvidia RTX 2080 Ti 11GB\n\n\nRAM\n32 GB\n\n\nOS\nDebian 12 (Bookwoorm) via WSL 2 (Windows 11 Pro 22H2)\n\n\nLanguage\nPython 3.11.2\n\n\n\n\n\n\n\nSetup Python Virtual Environment using venv.\nRegister to obtain right to download Meta - Llama2 models.\nRegister account in HuggingFace and download Llama2 model that is compatible to HF from https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\nInstall required python packages to perform inference using GPU\npip install bitsandbytes transformers accelerate optimum\nCode from HuggingFace and modified these 3 things:\n\nUse Llama2 models instead\nRemove BetterTransformer as LLama already included\nRemove FlashAttention2 code as Turing Achitecture not support\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# load model in 4-bit\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n# pre-downloaded model\nmodel_name = \"./Llama-2-7b-chat-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, \n                                                quantization_config=quantization_config)\n\ninput_text = \"Hello my dog is cute and\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nOutput\n\n\n\n\n\n\nhttps://huggingface.co/docs/transformers/en/perf_infer_gpu_one\nhttps://huggingface.co/docs/optimum/bettertransformer/overview"
  },
  {
    "objectID": "posts/llm_inference_using_rtx_2080_ti-part_1/index.html#overview",
    "href": "posts/llm_inference_using_rtx_2080_ti-part_1/index.html#overview",
    "title": "LLM Inference using RTX 2080 Ti (Part 1)",
    "section": "",
    "text": "First try on Large Language Model (LLM) inference coding. Pick Llama2 7b Chat HF model which can downloaded from HuggingFace after request from Meta. Inference hardware is using Nvidia RTX 2080 Ti and code in python language. Primary Python packages are from HuggingFace packages which are transformer, accelerate, optimum and bitsandbytes. Due to limitation of RTX 2080 Ti Turing architecture and available of GPU memory size, the model need to quantize to lower bit so that it can fit into 2080 Ti memory size and exclude of usage FlashAttention2 packages since it only support Ampere and newer."
  },
  {
    "objectID": "posts/llm_inference_using_rtx_2080_ti-part_1/index.html#environment",
    "href": "posts/llm_inference_using_rtx_2080_ti-part_1/index.html#environment",
    "title": "LLM Inference using RTX 2080 Ti (Part 1)",
    "section": "",
    "text": "Category\nModel\n\n\n\n\nCPU\nIntel i7-8700K\n\n\nGPU\nNvidia RTX 2080 Ti 11GB\n\n\nRAM\n32 GB\n\n\nOS\nDebian 12 (Bookwoorm) via WSL 2 (Windows 11 Pro 22H2)\n\n\nLanguage\nPython 3.11.2"
  },
  {
    "objectID": "posts/llm_inference_using_rtx_2080_ti-part_1/index.html#steps",
    "href": "posts/llm_inference_using_rtx_2080_ti-part_1/index.html#steps",
    "title": "LLM Inference using RTX 2080 Ti (Part 1)",
    "section": "",
    "text": "Setup Python Virtual Environment using venv.\nRegister to obtain right to download Meta - Llama2 models.\nRegister account in HuggingFace and download Llama2 model that is compatible to HF from https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\nInstall required python packages to perform inference using GPU\npip install bitsandbytes transformers accelerate optimum\nCode from HuggingFace and modified these 3 things:\n\nUse Llama2 models instead\nRemove BetterTransformer as LLama already included\nRemove FlashAttention2 code as Turing Achitecture not support\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# load model in 4-bit\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n# pre-downloaded model\nmodel_name = \"./Llama-2-7b-chat-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, \n                                                quantization_config=quantization_config)\n\ninput_text = \"Hello my dog is cute and\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nOutput"
  },
  {
    "objectID": "posts/llm_inference_using_rtx_2080_ti-part_1/index.html#references",
    "href": "posts/llm_inference_using_rtx_2080_ti-part_1/index.html#references",
    "title": "LLM Inference using RTX 2080 Ti (Part 1)",
    "section": "",
    "text": "https://huggingface.co/docs/transformers/en/perf_infer_gpu_one\nhttps://huggingface.co/docs/optimum/bettertransformer/overview"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FarngHwai.github.io",
    "section": "",
    "text": "LLM Inference using RTX 2080 Ti (Part 3)\n\n\n\n\n\n\nllm\n\n\nhf\n\n\nlangchain\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nFarng Hwai Yew\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Inference using RTX 2080 Ti (Part 2)\n\n\n\n\n\n\nllm\n\n\nhf\n\n\nlangchain\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nFarng Hwai Yew\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Inference using RTX 2080 Ti (Part 1)\n\n\n\n\n\n\nllm\n\n\nhf\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nFarng Hwai Yew\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nFarng Hwai Yew\n\n\n\n\n\n\nNo matching items"
  }
]